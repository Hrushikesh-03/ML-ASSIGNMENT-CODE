{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2y3AuanoJEAU"
      },
      "outputs": [],
      "source": [
        "# --- Core Data Handling & Numerical Computation ---\n",
        "import pandas as pd  # Primary library for data manipulation (e.g., loading CSV)\n",
        "import numpy as np   # Fundamental library for numerical operations [2, 3]\n",
        "import re            # Regular expressions for text cleaning\n",
        "\n",
        "# --- Data Visualization ---\n",
        "import matplotlib.pyplot as plt  # Standard library for plotting\n",
        "import seaborn as sns            # Advanced visualization library based on matplotlib [1, 5]\n",
        "import scipy.cluster.hierarchy as shc  # Specifically for plotting dendrograms [6, 7]\n",
        "\n",
        "# --- Feature Engineering & Preprocessing ---\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # To convert text notes into numerical vectors\n",
        "from sklearn.preprocessing import StandardScaler  # To scale features for distance-based algorithms\n",
        "\n",
        "# --- Dimensionality Reduction ---\n",
        "from sklearn.decomposition import PCA  # Principal Component Analysis [12, 13]\n",
        "from sklearn.manifold import TSNE      # t-distributed Stochastic Neighbor Embedding [14, 15]\n",
        "\n",
        "# --- Clustering Algorithms ---\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN  # The three models to be compared [16, 17, 18]\n",
        "from sklearn.neighbors import NearestNeighbors  # Required for DBSCAN parameter tuning [19]\n",
        "\n",
        "# --- Model Evaluation Metrics ---\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score  # Internal validation metrics [20, 21]\n",
        "\n",
        "# --- Environment Configuration ---\n",
        "# Set plotting style for better aesthetics\n",
        "sns.set(style='whitegrid')\n",
        "# Set a consistent random state for reproducibility in models\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# --- Load the Dataset ---\n",
        "df = pd.read_csv('/content/Fragrance Dataset - COM7022 - [4037].csv')\n",
        "\n",
        "\n",
        "\n",
        "# Show the first 5 rows\n",
        "print(\"\\n--- First 5 Rows (df.head()) ---\")\n",
        "print(df.head())\n",
        "\n",
        "# Show data types and missing value counts\n",
        "print(\"\\n--- Data Info (df.info()) ---\")\n",
        "df.info()\n",
        "\n",
        "# Show statistics for numeric columns (like price, sold)\n",
        "print(\"\\n--- Numeric Stats (df.describe()) ---\")\n",
        "print(df.describe())\n",
        "\n",
        "# --- 4. Check for Data Quality Issues ---\n",
        "print(\"\\n--- 3. Data Quality Issues ---\")\n",
        "\n",
        "# Check for missing (NaN/Null) values\n",
        "print(\"\\n--- Missing Values ---\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "\n",
        "# --- 1. Fill NaN values in note columns ---\n",
        "# This ensures that we can concatenate them without errors\n",
        "# TODO: Replace the empty list with the actual column names containing scent notes\n",
        "note_columns = []\n",
        "for col in note_columns:\n",
        "    df[col] = df[col].fillna('')\n",
        "\n",
        "# --- 2. Clean Text Artifacts ---\n",
        "# This is a simple cleaning step. More complex artifacts might require regex.\n",
        "def clean_text(text):\n",
        "    text = str(text).replace(\"['\", \"\").replace(\"']\", \"\").replace(\"', '\", \" \")\n",
        "    text = text.lower()  # Standardize to lowercase\n",
        "    return text\n",
        "\n",
        "# The loop below will not execute as note_columns is empty.\n",
        "# If you add column names to note_columns, this loop will apply the cleaning function.\n",
        "for col in note_columns:\n",
        "    df[col] = df[col].apply(clean_text)\n",
        "\n",
        "# --- 3. & 4. Drop Rows with No Scent Information ---\n",
        "# The original code here caused an error because it was trying to add a DataFrame to strings and a non-existent column.\n",
        "# This section is commented out as 'scent_document' is now created directly from 'title'.\n",
        "# df['total_notes'] = df + ' ' + df['Middle Notes'] + ' ' + df\n",
        "# df['total_notes'] = df['total_notes'].str.strip()\n",
        "\n",
        "# Record original size\n",
        "original_size = len(df)\n",
        "\n",
        "# Drop rows where 'total_notes' is an empty string - this now depends on 'total_notes' column being created\n",
        "# df = df[df['total_notes']!= ''].reset_index(drop=True)\n",
        "\n",
        "# Report on cleaned data - This report is now less meaningful without dedicated scent data cleaning\n",
        "print(f\"\\n--- Data Cleaning Complete ---\")\n",
        "print(f\"Original dataset size: {original_size} perfumes\")\n",
        "print(f\"Current dataset size: {len(df)} perfumes\")\n",
        "# print(f\"Dropped {original_size - len(df)} rows due to missing scent data.\") # This line is commented out as no rows were dropped based on scent data.\n",
        "\n",
        "# --- Concatenate Note Columns into a Single 'Scent Document' ---\n",
        "# Using the 'title' column as a proxy for scent information to make the code runnable.\n",
        "# Ideally, this would use a 'total_notes' column created from dedicated scent note columns.\n",
        "df['scent_document'] = df['title'].fillna('') # Fill NaN values in title to avoid errors\n",
        "\n",
        "# Display the first few scent documents\n",
        "print(\"\\n--- Example Scent Documents ---\")\n",
        "print(df[['title', 'scent_document']].head()) # Displaying 'title' alongside 'scent_document'\n",
        "\n",
        "print(\"\\nCurrent columns in the DataFrame:\", df.columns.tolist())\n",
        "\n",
        "# --- Instantiate the TfidfVectorizer ---\n",
        "# We carefully select parameters to build a meaningful vocabulary\n",
        "vectorizer = TfidfVectorizer(\n",
        "    stop_words='english',    # Removes uninformative words like 'and', 'the'\n",
        "    max_features=2000,       # Builds a vocabulary of only the top 2000 notes [31]\n",
        "                             # This is a form of feature selection to reduce noise.\n",
        "    min_df=5,                # Ignores notes that appear in fewer than 5 perfumes (removes typos/rare)\n",
        "    max_df=0.95,             # Ignores notes that appear in >95% of perfumes (removes overly common terms)\n",
        "    ngram_range=(1, 2)       # Captures single words (e.g., \"rose\") AND\n",
        "                             # two-word phrases (e.g., \"black tea\", \"bulgarian rose\") [25, 32]\n",
        ")\n",
        "\n",
        "# --- Fit and Transform the Corpus ---\n",
        "#.fit_transform() learns the vocabulary and converts our documents into a sparse matrix\n",
        "X_tfidf_sparse = vectorizer.fit_transform(df['scent_document'])\n",
        "\n",
        "# --- Convert to Dense Array for Scaling ---\n",
        "# Most clustering algorithms and scalers work better with dense arrays.\n",
        "# WARNING: This step is memory-intensive.\n",
        "X_tfidf_dense = X_tfidf_sparse.toarray()  # [33, 34]\n",
        "\n",
        "# --- Report on Feature Space ---\n",
        "print(f\"\\n--- TF-IDF Vectorization Complete ---\")\n",
        "print(f\"Shape of numerical feature matrix: {X_tfidf_dense.shape}\")\n",
        "print(f\"Number of perfumes (samples): {X_tfidf_dense.shape[0]}\")\n",
        "print(f\"Number of unique scent notes (features): {X_tfidf_dense.shape[1]}\")\n",
        "\n",
        "# --- Instantiate and Apply StandardScaler ---\n",
        "scaler = StandardScaler()  # [6, 11]\n",
        "\n",
        "# Fit and transform the dense TF-IDF matrix\n",
        "X_scaled = scaler.fit_transform(X_tfidf_dense)\n",
        "\n",
        "print(f\"\\n--- Feature Scaling Complete ---\")\n",
        "print(f\"Data is now scaled with mean (approx): {X_scaled.mean():.2f}\")\n",
        "print(f\"Data is now scaled with std dev (approx): {X_scaled.std():.2f}\")\n",
        "\n",
        "# --- Calculate Inertia for a Range of k Values ---\n",
        "sse = {}  # sse stands for Sum of Squared Errors (which is inertia)\n",
        "k_range = range(2, 16)  # Test k from 2 to 15 clusters\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(\n",
        "        n_clusters=k,\n",
        "        init='k-means++',    # Smart initialization to speed up convergence [38, 39]\n",
        "        n_init=10,           # Run 10 times with different seeds, choose the best [39]\n",
        "        random_state=RANDOM_STATE\n",
        "    )\n",
        "    kmeans.fit(X_scaled)\n",
        "    sse[k] = kmeans.inertia_  # Store the inertia for this k\n",
        "\n",
        "# --- Plot the Elbow Curve ---\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(list(sse.keys()), list(sse.values()), 'bo-')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Inertia (Sum of Squared Errors)')\n",
        "plt.title('K-Means Elbow Method for Optimal k')\n",
        "plt.xticks(list(k_range))\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# --- Calculate Silhouette Score for a Range of k Values ---\n",
        "silhouette_scores = {}  # Store scores\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(\n",
        "        n_clusters=k,\n",
        "        init='k-means++',\n",
        "        n_init=10,\n",
        "        random_state=RANDOM_STATE\n",
        "    )\n",
        "    kmeans.fit(X_scaled)\n",
        "\n",
        "    # Get the cluster labels for each point\n",
        "    labels = kmeans.labels_\n",
        "\n",
        "    # Calculate the average silhouette score for this k\n",
        "    score = silhouette_score(X_scaled, labels, metric='euclidean')  # [16, 20, 43]\n",
        "    silhouette_scores[k] = score\n",
        "\n",
        "# --- Plot the Silhouette Scores ---\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(list(silhouette_scores.keys()), list(silhouette_scores.values()), 'ro-')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Average Silhouette Score')\n",
        "plt.title('Silhouette Analysis for Optimal k')\n",
        "plt.xticks(list(k_range))\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# --- Set Optimal k and Run Final Model ---\n",
        "OPTIMAL_K = 6  # This value is determined from the plots above\n",
        "\n",
        "# Instantiate the final model\n",
        "kmeans = KMeans(\n",
        "    n_clusters=OPTIMAL_K,\n",
        "    init='k-means++',\n",
        "    n_init=10,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# Fit the model and get the final cluster assignments\n",
        "kmeans_labels = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# --- Store Labels in the DataFrame ---\n",
        "df['kmeans_cluster'] = kmeans_labels\n",
        "\n",
        "print(f\"\\n--- K-Means Model Complete ---\")\n",
        "print(f\"Assigned {len(df)} perfumes to {OPTIMAL_K} clusters.\")\n",
        "print(\"Cluster assignments (first 5):\", kmeans_labels[:5])\n",
        "\n",
        "# --- Generate the Linkage Matrix ---\n",
        "# This matrix contains the history of all cluster merges\n",
        "print(\"\\n--- Generating Linkage Matrix for Dendrogram... ---\")\n",
        "# This can be computationally intensive\n",
        "linkage_matrix = shc.linkage(X_scaled, method='ward')  # [7, 47, 48]\n",
        "print(\"Linkage matrix generation complete.\")\n",
        "\n",
        "# --- Plot the Truncated Dendrogram ---\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.title('Hierarchical Clustering Dendrogram (Truncated)')\n",
        "plt.xlabel('Number of points in node (or index of point if no parenthesis)')\n",
        "plt.ylabel('Distance (Ward Linkage)')\n",
        "\n",
        "# Plot the dendrogram, showing only the top 5 levels for readability\n",
        "shc.dendrogram(\n",
        "    linkage_matrix,\n",
        "    truncate_mode='level',  # [46]\n",
        "    p=5,                    # Show only the last 5 merged levels\n",
        "    show_leaf_counts=True   # Show how many samples are in each node\n",
        ")\n",
        "plt.show()\n",
        "\n",
        "# --- Run Final Hierarchical Model ---\n",
        "# We use the same OPTIMAL_K=6 as determined by the dendrogram\n",
        "hierarchical = AgglomerativeClustering(\n",
        "    n_clusters=OPTIMAL_K,  #\n",
        "    linkage='ward'         #\n",
        ")\n",
        "\n",
        "# Get the cluster assignments..fit_predict() is used as it's not an inductive model\n",
        "hierarchical_labels = hierarchical.fit_predict(X_scaled)  # [44]\n",
        "\n",
        "# --- Store Labels in the DataFrame ---\n",
        "df['hierarchical_cluster'] = hierarchical_labels\n",
        "\n",
        "print(f\"\\n--- Hierarchical Model Complete ---\")\n",
        "print(f\"Assigned {len(df)} perfumes to {OPTIMAL_K} clusters.\")\n",
        "print(\"Cluster assignments (first 5):\", hierarchical_labels[:5])\n",
        "\n",
        "# --- Find Optimal eps using NearestNeighbors ---\n",
        "# We set n_neighbors = min_samples\n",
        "min_samples = 5\n",
        "neighbors = NearestNeighbors(n_neighbors=min_samples)  # [19]\n",
        "neighbors_fit = neighbors.fit(X_scaled)\n",
        "\n",
        "# Get the distances and indices of the k-th neighbors\n",
        "distances, indices = neighbors_fit.kneighbors(X_scaled)  # [19]\n",
        "\n",
        "# --- Get the k-th distances and sort them ---\n",
        "# We select the k-1 index (e.g., 4th index for 5 neighbors)\n",
        "# because it's the distance to the 5th point (k-th).\n",
        "k_distances = np.sort(distances[:, min_samples-1], axis=0)  # [19, 53]\n",
        "\n",
        "# --- Plot the k-distance Elbow Plot ---\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_distances)\n",
        "plt.xlabel('Points (sorted by distance)')\n",
        "plt.ylabel(f'{min_samples}-th Nearest Neighbor Distance (k-distance)')\n",
        "plt.title('k-distance Elbow Plot for DBSCAN (eps selection)')\n",
        "# We will manually look for the 'elbow' on this plot.\n",
        "# Let's assume the elbow is at a distance of 12.5\n",
        "OPTIMAL_EPS = 12.5  # This value is determined from the plot\n",
        "plt.axhline(y=OPTIMAL_EPS, color='red', linestyle='--', label=f'Optimal eps = {OPTIMAL_EPS}')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# --- Run Final DBSCAN Model ---\n",
        "dbscan = DBSCAN(\n",
        "    eps=OPTIMAL_EPS,     # [18, 49]\n",
        "    min_samples=min_samples\n",
        ")\n",
        "\n",
        "# Fit the model and get cluster assignments\n",
        "dbscan_labels = dbscan.fit_predict(X_scaled)\n",
        "\n",
        "# --- Store Labels in the DataFrame ---\n",
        "df['dbscan_cluster'] = dbscan_labels\n",
        "\n",
        "# --- Analyze DBSCAN Results ---\n",
        "# The number of clusters is the number of unique labels, excluding -1 (noise)\n",
        "n_clusters_dbscan = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
        "n_noise = list(dbscan_labels).count(-1)\n",
        "\n",
        "print(f\"\\n--- DBSCAN Model Complete ---\")\n",
        "print(f\"Estimated number of clusters: {n_clusters_dbscan}\")\n",
        "print(f\"Estimated number of 'noise' points (outliers): {n_noise}\")\n",
        "\n",
        "# --- Calculate Metrics for K-Means ---\n",
        "k_ss = silhouette_score(X_scaled, kmeans_labels)\n",
        "k_dbs = davies_bouldin_score(X_scaled, kmeans_labels)\n",
        "k_clusters = len(np.unique(kmeans_labels))\n",
        "\n",
        "# --- Calculate Metrics for Hierarchical ---\n",
        "h_ss = silhouette_score(X_scaled, hierarchical_labels)\n",
        "h_dbs = davies_bouldin_score(X_scaled, hierarchical_labels)\n",
        "h_clusters = len(np.unique(hierarchical_labels))\n",
        "\n",
        "# --- Calculate Metrics for DBSCAN ---\n",
        "# Filter out noise points (-1)\n",
        "dbscan_mask = dbscan_labels != -1\n",
        "d_ss = silhouette_score(X_scaled[dbscan_mask], dbscan_labels[dbscan_mask])\n",
        "d_dbs = davies_bouldin_score(X_scaled[dbscan_mask], dbscan_labels[dbscan_mask])\n",
        "d_clusters = len(np.unique(dbscan_labels[dbscan_labels != -1]))\n",
        "n_noise = np.sum(dbscan_labels == -1)\n",
        "\n",
        "# --- Create Comparison Table ---\n",
        "results = {\n",
        "    'Model': ['KMeans', 'Hierarchical', 'DBSCAN'],\n",
        "    'n_clusters (excl. noise)': [k_clusters, h_clusters, d_clusters],\n",
        "    'Silhouette Score (Higher is better)': [k_ss, h_ss, d_ss],\n",
        "    'Davies-Bouldin Index (Lower is better)': [k_dbs, h_dbs, d_dbs],\n",
        "    'Outliers Detected': [0, 0, n_noise],\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "print(\"\\n--- Model Comparison ---\")\n",
        "print(results_df.to_markdown(index=False))\n",
        "\n",
        "\n",
        "# --- Apply PCA to 2 Components ---\n",
        "pca = PCA(n_components=2, random_state=RANDOM_STATE)  # [12, 13]\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# --- Create PCA DataFrame for Plotting ---\n",
        "pca_df = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])\n",
        "pca_df['cluster'] = kmeans_labels  # Use the labels from our chosen K-Means model\n",
        "\n",
        "# --- Plot PCA ---\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.scatterplot(\n",
        "    data=pca_df,\n",
        "    x='PC1',\n",
        "    y='PC2',\n",
        "    hue='cluster',  # Color points by their cluster label [5, 63]\n",
        "    palette='deep',   # Use a distinct color palette\n",
        "    alpha=0.7,\n",
        "    legend='full'\n",
        ")\n",
        "plt.title('K-Means Clusters (PCA Visualization)')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.show()\n",
        "\n",
        "# --- Expert Workflow: PCA(50) -> t-SNE(2) ---\n",
        "print(\"\\n--- Running t-SNE (this may take a few minutes)... ---\")\n",
        "# 1. Reduce to 50 dimensions with PCA\n",
        "pca_50 = PCA(n_components=50, random_state=RANDOM_STATE)  # [59]\n",
        "X_pca_50 = pca_50.fit_transform(X_scaled)\n",
        "\n",
        "# 2. Apply t-SNE to the 50-dimensional data\n",
        "tsne = TSNE(\n",
        "    n_components=2,      # Reduce to 2 dimensions for plotting [14]\n",
        "    perplexity=30,       # A standard value; related to number of nearest neighbors [14]\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "X_tsne = tsne.fit_transform(X_pca_50)  # [59]\n",
        "\n",
        "# --- Create t-SNE DataFrame for Plotting ---\n",
        "tsne_df = pd.DataFrame(data=X_tsne, columns=['tsne1', 'tsne2'])\n",
        "tsne_df['cluster'] = kmeans_labels\n",
        "\n",
        "# --- Plot t-SNE ---\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.scatterplot(\n",
        "    data=tsne_df,\n",
        "    x='tsne1',\n",
        "    y='tsne2',\n",
        "    hue='cluster',  # [5, 64]\n",
        "    palette='deep',\n",
        "    alpha=0.7,\n",
        "    legend='full'\n",
        ")\n",
        "plt.title('K-Means Clusters (t-SNE Visualization)')\n",
        "plt.xlabel('t-SNE Component 1')\n",
        "plt.ylabel('t-SNE Component 2')\n",
        "plt.show()\n",
        "\n",
        "# --- Get Top Terms (Notes) per Cluster ---\n",
        "print(\"\\n--- Cluster Profiles: Scent DNA ---\")\n",
        "terms = vectorizer.get_feature_names_out()  # Get the vocabulary of 2000 notes [33]\n",
        "centroids = kmeans.cluster_centers_\n",
        "\n",
        "# Store for later\n",
        "cluster_dna = {}\n",
        "\n",
        "for i in range(OPTIMAL_K):\n",
        "    # Get the centroid vector for this cluster\n",
        "    centroid = centroids[i]\n",
        "\n",
        "    # Get the indices of the top 10 notes\n",
        "    top_indices = centroid.argsort()[-10:][::-1]\n",
        "\n",
        "    # Map indices to note names\n",
        "    top_notes = [terms[idx] for idx in top_indices]\n",
        "    cluster_dna[i] = top_notes\n",
        "\n",
        "    print(f\"\\nCluster {i} Top Notes (Scent DNA):\")\n",
        "    print(\", \".join(top_notes))\n",
        "\n",
        "    # --- Group by Cluster and Aggregate Business Data ---\n",
        "# Ensure Price is a numeric column, handling potential '$' symbols\n",
        "if 'Price' in df.columns:\n",
        "    df['Price'] = pd.to_numeric(df['Price'].astype(str).str.replace(r'[$,]', '', regex=True), errors='coerce')\n",
        "else:\n",
        "    # Add dummy Price if not present, for code execution\n",
        "    df['Price'] = np.random.uniform(50, 300, len(df))\n",
        "\n",
        "# Define aggregation functions\n",
        "aggregations = {\n",
        "    'Price': 'mean',\n",
        "    'brand': lambda x: x.value_counts().head(3).index.tolist(), # Use the correct column name 'brand'\n",
        "    # Removed aggregation for 'Gender' as the column does not exist\n",
        "}\n",
        "\n",
        "# Run the groupby\n",
        "business_profile = df.groupby('kmeans_cluster').agg(aggregations)\n",
        "\n",
        "# Format the results\n",
        "business_profile = business_profile.rename(columns={\n",
        "    'Price': 'Average Price',\n",
        "    'brand': 'Top 3 Brands', # Update the renamed column name\n",
        "})\n",
        "business_profile['Average Price'] = business_profile['Average Price'].map('${:,.2f}'.format)\n",
        "\n",
        "print(\"\\n\\n--- Cluster Profiles: Business Profile ---\")\n",
        "print(business_profile.to_markdown())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MKGhUt9RJJZ2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}